{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_DIM = 4\n",
    "GRID_SIZE = GRID_DIM * GRID_DIM\n",
    "A, A_ , B, B_ = np.array([0, 1]), np.array([4, 1]), np.array([0, 3]), np.array([2, 3])\n",
    "LEFT, RIGHT, UP , DOWN = np.array([0, -1]), np.array([0, 1]), np.array([-1, 0]), \\\n",
    "                        np.array([1, 0]) \n",
    "A_REWARD = +5.0\n",
    "B_REWARD = +10.0\n",
    "# GAMMA = 0.9 \n",
    "GAMMA = 1.\n",
    "Pi_as = 0.25\n",
    "ITERATIONS = 10000\n",
    "ACTIONS = [LEFT, RIGHT , UP , DOWN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(location):\n",
    "    if location[0] == 0 and location[0] == 0 :\n",
    "        return 0\n",
    "    if location[0] == GRID_DIM-1 and location[0] == GRID_DIM-1:\n",
    "        return 0\n",
    "    \n",
    "    return -1\n",
    "def get_state_reward(cur_state, action):\n",
    "    if cur_state[0] == 0 and cur_state[1] == 0:\n",
    "        return cur_state, 0  \n",
    "    if cur_state[0] == GRID_DIM-1 and cur_state[1] == GRID_DIM-1:\n",
    "        return cur_state, 0\n",
    "    \n",
    "    n_loc = cur_state + action\n",
    "    if n_loc[0] >= GRID_DIM or n_loc[0] < 0 or n_loc[1] >= GRID_DIM or n_loc[1] < 0:\n",
    "        return cur_state, -1\n",
    "    else:\n",
    "        return n_loc, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "[[ 0. -2. -2. -2.]\n",
      " [-2. -2. -2. -2.]\n",
      " [-2. -2. -2. -2.]\n",
      " [-2. -2. -2.  0.]]\n",
      "[[ 0. -3. -3. -3.]\n",
      " [-3. -3. -3. -3.]\n",
      " [-3. -3. -3. -3.]\n",
      " [-3. -3. -3.  0.]]\n"
     ]
    }
   ],
   "source": [
    "grid_world_v_pi = np.zeros((GRID_DIM, GRID_DIM))\n",
    "for k in range(3):\n",
    "    for i in range(GRID_DIM):\n",
    "        for j in range(GRID_DIM):\n",
    "            cur_action = ACTIONS[np.random.randint(0,3)]\n",
    "\n",
    "            new_state, reward = get_state_reward(np.array([i, j]), cur_action)\n",
    "            grid_world_v_pi[i,j] += GAMMA**k * reward\n",
    "    print(grid_world_v_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K  0\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "Policy\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "K  1\n",
      "[[ 0.  -1.8 -2.  -2. ]\n",
      " [-1.8 -2.  -2.  -2. ]\n",
      " [-2.  -2.  -2.  -1.8]\n",
      " [-2.  -2.  -1.8  0. ]]\n",
      "Policy\n",
      "[[0. 0. 0. 0.]\n",
      " [2. 0. 0. 0.]\n",
      " [0. 0. 0. 3.]\n",
      " [0. 0. 1. 0.]]\n",
      "K  2\n",
      "[[ 0.  -2.4 -2.9 -3. ]\n",
      " [-2.4 -2.9 -3.  -2.9]\n",
      " [-2.9 -3.  -2.9 -2.4]\n",
      " [-3.  -2.9 -2.4  0. ]]\n",
      "Policy\n",
      "[[0. 0. 0. 0.]\n",
      " [2. 0. 0. 3.]\n",
      " [2. 0. 1. 3.]\n",
      " [0. 1. 1. 0.]]\n",
      "K  3\n",
      "[[ 0.  -3.1 -3.8 -4. ]\n",
      " [-3.1 -3.7 -3.9 -3.8]\n",
      " [-3.8 -3.9 -3.7 -3.1]\n",
      " [-4.  -3.8 -3.1  0. ]]\n",
      "Policy\n",
      "[[0. 0. 0. 0.]\n",
      " [2. 0. 0. 3.]\n",
      " [2. 1. 1. 3.]\n",
      " [1. 1. 1. 0.]]\n",
      "K  4\n",
      "[[ 0.  -3.7 -4.7 -4.9]\n",
      " [-3.7 -4.5 -4.8 -4.7]\n",
      " [-4.7 -4.8 -4.5 -3.7]\n",
      " [-4.9 -4.7 -3.7  0. ]]\n",
      "Policy\n",
      "[[0. 0. 0. 0.]\n",
      " [2. 0. 0. 3.]\n",
      " [2. 1. 1. 3.]\n",
      " [1. 1. 1. 0.]]\n",
      "K  10\n",
      "[[ 0.  -6.6 -9.  -9.7]\n",
      " [-6.6 -8.3 -9.  -9. ]\n",
      " [-9.  -9.  -8.3 -6.6]\n",
      " [-9.7 -9.  -6.6  0. ]]\n",
      "Policy\n",
      "[[0. 0. 0. 0.]\n",
      " [2. 0. 0. 3.]\n",
      " [2. 1. 1. 3.]\n",
      " [1. 1. 1. 0.]]\n",
      "K  20\n",
      "[[  0.   -9.7 -13.6 -14.9]\n",
      " [ -9.7 -12.4 -13.7 -13.6]\n",
      " [-13.6 -13.7 -12.4  -9.7]\n",
      " [-14.9 -13.6  -9.7   0. ]]\n",
      "Policy\n",
      "[[0. 0. 0. 0.]\n",
      " [2. 0. 0. 3.]\n",
      " [2. 1. 1. 3.]\n",
      " [1. 1. 1. 0.]]\n",
      "K  30\n",
      "[[  0.  -11.5 -16.3 -17.9]\n",
      " [-11.5 -14.7 -16.3 -16.3]\n",
      " [-16.3 -16.3 -14.7 -11.5]\n",
      " [-17.9 -16.3 -11.5   0. ]]\n",
      "Policy\n",
      "[[0. 0. 0. 0.]\n",
      " [2. 0. 0. 3.]\n",
      " [2. 1. 1. 3.]\n",
      " [1. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "v_pi = np.zeros((GRID_DIM, GRID_DIM))\n",
    "count = 0\n",
    "# value_grid \n",
    "for k in range(40):\n",
    "    v_pi_ = np.zeros((GRID_DIM, GRID_DIM))\n",
    "    v_action = np.zeros((GRID_DIM, GRID_DIM))\n",
    "    for i in range(GRID_DIM):\n",
    "        for j in range(GRID_DIM):\n",
    "            max_action = None\n",
    "            max_value = None\n",
    "            actions_grid = [] \n",
    "            for index, act in enumerate(ACTIONS):\n",
    "                cur_state = np.array([i, j])\n",
    "                (n_i, n_j), reward = get_state_reward(cur_state, act)\n",
    "                # equation 3.12\n",
    "                v_pi_[i, j] += Pi_as * (reward + GAMMA * v_pi[n_i, n_j])\n",
    "                if index == 0:\n",
    "                    max_value = Pi_as * (reward + GAMMA * v_pi[n_i, n_j])\n",
    "                    max_action = index\n",
    "                else:\n",
    "                    if max_value < Pi_as * (reward + GAMMA * v_pi[n_i, n_j]):\n",
    "                        max_action = index\n",
    "                        max_value = Pi_as * (reward + GAMMA * v_pi[n_i, n_j])\n",
    "                v_action[i,j] = max_action\n",
    "                \n",
    "                \n",
    "                \n",
    "    count += 1\n",
    "    if np.sum(np.abs(v_pi - v_pi_)) < 1e-4:\n",
    "        break\n",
    "    v_pi = v_pi_\n",
    "    if k < 5 or (k > 5 and k % 10==0):\n",
    "        print('K ', k)\n",
    "        print(np.round(v_pi, decimals=1))\n",
    "        print('Policy')            \n",
    "        print(v_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Book Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_state_reward(cur_state, action):\n",
    "    if cur_state[0] == 0 and cur_state[1] == 0:\n",
    "        return cur_state, 0  \n",
    "    if cur_state[0] == GRID_DIM-1 and cur_state[1] == GRID_DIM-1:\n",
    "        return cur_state, 0\n",
    "    n_loc = np.zeros(2)\n",
    "\n",
    "    n_loc[0] = cur_state[0] + (action[0])\n",
    "    n_loc[1] = cur_state[1] + action[1]\n",
    "    n_loc = n_loc.astype(int)\n",
    "\n",
    "    if n_loc[0] >= GRID_DIM or n_loc[0] < 0 or n_loc[1] >= GRID_DIM or n_loc[1] < 0:\n",
    "        return cur_state, -1\n",
    "    else:\n",
    "        return n_loc, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_DIM  = 4\n",
    "LEFT, RIGHT, UP , DOWN = np.array([0, -1]), np.array([0, 1]), \\\n",
    "                        np.array([-1, 0]), \\\n",
    "                        np.array([1, 0]) \n",
    "ACTIONS = [LEFT, RIGHT, UP , DOWN]\n",
    "\n",
    "Pi_as = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 False\n",
      "Iteration 1 False\n",
      "Iteration 2 False\n",
      "Iteration 3 False\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "[[0 0 0 0]\n",
      " [2 0 0 3]\n",
      " [2 0 1 3]\n",
      " [1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "V_s = np.zeros((GRID_DIM, GRID_DIM))\n",
    "pi_ = np.zeros((GRID_DIM, GRID_DIM)).astype(int)\n",
    "\n",
    "policy_stable = False\n",
    "count = 0\n",
    "while not policy_stable:\n",
    "    print('Iteration', count, policy_stable)\n",
    "    policy_stable = True\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for i in range(GRID_DIM):\n",
    "            for j in range(GRID_DIM):\n",
    "                if (i,j)  != (0,0) and (i,j)!=(3,3):\n",
    "                    v = V_s[i,j]\n",
    "                    cur_loc = np.array([i, j])\n",
    "                    action = ACTIONS[pi_[i, j]]\n",
    "                \n",
    "                    next_state, rew = get_state_reward(cur_loc, action )\n",
    "                    V_s[i,j] = rew + V_s[next_state[0], next_state[1]]\n",
    "\n",
    "                    delta = max(delta, abs(v - V_s[i,j]))\n",
    "#                 print(delta)\n",
    "        if delta < 3.:\n",
    "            break\n",
    "\n",
    "    for i in range(GRID_DIM):\n",
    "        for j in range(GRID_DIM):\n",
    "            old_action = pi_[i, j]\n",
    "            values = []\n",
    "            cur_loc = np.array([i,j])\n",
    "            for action in ACTIONS:\n",
    "                st, rew = get_state_reward(cur_loc, action)\n",
    "                values.append(rew + V_s[st[0], st[1]])\n",
    "            \n",
    "            pi_[i,j] = np.argmax(values)\n",
    "            if old_action != pi_[i, j]:\n",
    "                policy_stable = False\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "print(V_s)\n",
    "print(pi_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dash = []\n",
    "for i in range(GRID_DIM):\n",
    "    for j in range(GRID_DIM):\n",
    "        old_action = pi_[i, j]\n",
    "        values = []\n",
    "        cur_loc = np.array([i,j])\n",
    "        \n",
    "        vp  = []\n",
    "        for action in ACTIONS:\n",
    "            st, rew = get_state_reward(cur_loc, action)\n",
    "            values.append(rew + V_s[st[0], st[1]])\n",
    "        max_value = max(values)\n",
    "        for i , value in enumerate(values):\n",
    "            if value == max_value:\n",
    "                vp.append(i)\n",
    "        \n",
    "        p_dash.append(vp)\n",
    "#         pi_[i,j] = np.argmax(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0, 1, 2, 3] | [1] | [1] | [0, 1, 2, 3] | \n",
      "[2] | [1] | [1] | [0, 1, 2, 3] | \n",
      "[2] | [1] | [1] | [0, 1, 2, 3] | \n",
      "[1, 2] | [1] | [1] | [0, 1, 2, 3] | "
     ]
    }
   ],
   "source": [
    "for i in range(GRID_DIM):\n",
    "    print()\n",
    "    for j in range(GRID_DIM):\n",
    "        print(p_dash[i*GRID_DIM + j], '|' , end =\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K  0\n",
      "[[ 0.   -0.25 -0.25 -0.25]\n",
      " [-0.25 -0.25 -0.25 -0.25]\n",
      " [-0.25 -0.25 -0.25 -0.25]\n",
      " [-0.25 -0.25 -0.25  0.  ]]\n",
      "Policy\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "K  1\n",
      "[[ 0.     -0.25   -0.3125 -0.3125]\n",
      " [-0.25   -0.3125 -0.3125 -0.3125]\n",
      " [-0.3125 -0.3125 -0.3125 -0.25  ]\n",
      " [-0.3125 -0.3125 -0.25    0.    ]]\n",
      "Policy\n",
      "[[0. 0. 0. 0.]\n",
      " [2. 0. 0. 0.]\n",
      " [0. 0. 0. 3.]\n",
      " [0. 0. 1. 0.]]\n",
      "K  2\n",
      "[[ 0.       -0.25     -0.3125   -0.328125]\n",
      " [-0.25     -0.3125   -0.328125 -0.3125  ]\n",
      " [-0.3125   -0.328125 -0.3125   -0.25    ]\n",
      " [-0.328125 -0.3125   -0.25      0.      ]]\n",
      "Policy\n",
      "[[0. 0. 0. 0.]\n",
      " [2. 0. 0. 3.]\n",
      " [2. 0. 1. 3.]\n",
      " [0. 1. 1. 0.]]\n",
      "K  3\n",
      "[[ 0.       -0.25     -0.3125   -0.328125]\n",
      " [-0.25     -0.3125   -0.328125 -0.3125  ]\n",
      " [-0.3125   -0.328125 -0.3125   -0.25    ]\n",
      " [-0.328125 -0.3125   -0.25      0.      ]]\n",
      "Policy\n",
      "[[0. 0. 0. 0.]\n",
      " [2. 0. 0. 3.]\n",
      " [2. 0. 1. 3.]\n",
      " [1. 1. 1. 0.]]\n",
      "K  4\n",
      "[[ 0.       -0.25     -0.3125   -0.328125]\n",
      " [-0.25     -0.3125   -0.328125 -0.3125  ]\n",
      " [-0.3125   -0.328125 -0.3125   -0.25    ]\n",
      " [-0.328125 -0.3125   -0.25      0.      ]]\n",
      "Policy\n",
      "[[0. 0. 0. 0.]\n",
      " [2. 0. 0. 3.]\n",
      " [2. 0. 1. 3.]\n",
      " [1. 1. 1. 0.]]\n",
      "K  5\n",
      "[[ 0.       -0.25     -0.3125   -0.328125]\n",
      " [-0.25     -0.3125   -0.328125 -0.3125  ]\n",
      " [-0.3125   -0.328125 -0.3125   -0.25    ]\n",
      " [-0.328125 -0.3125   -0.25      0.      ]]\n",
      "Policy\n",
      "[[0. 0. 0. 0.]\n",
      " [2. 0. 0. 3.]\n",
      " [2. 0. 1. 3.]\n",
      " [1. 1. 1. 0.]]\n",
      "K  6\n",
      "[[ 0.       -0.25     -0.3125   -0.328125]\n",
      " [-0.25     -0.3125   -0.328125 -0.3125  ]\n",
      " [-0.3125   -0.328125 -0.3125   -0.25    ]\n",
      " [-0.328125 -0.3125   -0.25      0.      ]]\n",
      "Policy\n",
      "[[0. 0. 0. 0.]\n",
      " [2. 0. 0. 3.]\n",
      " [2. 0. 1. 3.]\n",
      " [1. 1. 1. 0.]]\n",
      "K  7\n",
      "[[ 0.       -0.25     -0.3125   -0.328125]\n",
      " [-0.25     -0.3125   -0.328125 -0.3125  ]\n",
      " [-0.3125   -0.328125 -0.3125   -0.25    ]\n",
      " [-0.328125 -0.3125   -0.25      0.      ]]\n",
      "Policy\n",
      "[[0. 0. 0. 0.]\n",
      " [2. 0. 0. 3.]\n",
      " [2. 0. 1. 3.]\n",
      " [1. 1. 1. 0.]]\n",
      "K  8\n",
      "[[ 0.       -0.25     -0.3125   -0.328125]\n",
      " [-0.25     -0.3125   -0.328125 -0.3125  ]\n",
      " [-0.3125   -0.328125 -0.3125   -0.25    ]\n",
      " [-0.328125 -0.3125   -0.25      0.      ]]\n",
      "Policy\n",
      "[[0. 0. 0. 0.]\n",
      " [2. 0. 0. 3.]\n",
      " [2. 0. 1. 3.]\n",
      " [1. 1. 1. 0.]]\n",
      "K  9\n",
      "[[ 0.       -0.25     -0.3125   -0.328125]\n",
      " [-0.25     -0.3125   -0.328125 -0.3125  ]\n",
      " [-0.3125   -0.328125 -0.3125   -0.25    ]\n",
      " [-0.328125 -0.3125   -0.25      0.      ]]\n",
      "Policy\n",
      "[[0. 0. 0. 0.]\n",
      " [2. 0. 0. 3.]\n",
      " [2. 0. 1. 3.]\n",
      " [1. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# experimental method\n",
    "v_pi = np.zeros((GRID_DIM, GRID_DIM))\n",
    "count = 0\n",
    "GAMMA = 1.\n",
    "for k in range(10):\n",
    "    v_pi_ = np.zeros((GRID_DIM, GRID_DIM))\n",
    "    v_action = np.zeros((GRID_DIM, GRID_DIM))\n",
    "    for i in range(GRID_DIM):\n",
    "        for j in range(GRID_DIM):\n",
    "            max_action = None\n",
    "            max_value = None\n",
    "            actions_grid = []\n",
    "            values = [] \n",
    "            for index, act in enumerate(ACTIONS):\n",
    "                cur_state = np.array([i, j])\n",
    "                (n_i, n_j), reward = get_state_reward(cur_state, act)\n",
    "                # equation 3.12\n",
    "                values.append(Pi_as * (reward + GAMMA * v_pi[n_i, n_j]))\n",
    "                if index == 0:\n",
    "                    max_value = Pi_as * (reward + GAMMA * v_pi[n_i, n_j])\n",
    "                    max_action = index\n",
    "                else:\n",
    "                    if max_value < Pi_as * (reward + GAMMA * v_pi[n_i, n_j]):\n",
    "                        max_action = index\n",
    "                        max_value = Pi_as * (reward + GAMMA * v_pi[n_i, n_j])\n",
    "                v_action[i,j] = max_action\n",
    "            v_pi_[i, j] = max(values)\n",
    "                \n",
    "                \n",
    "    count += 1\n",
    "#     if np.sum(np.abs(v_pi - v_pi_)) < 1e-4:\n",
    "#         break\n",
    "    v_pi = v_pi_\n",
    "    print('K ', k)\n",
    "    print(v_pi)\n",
    "    print('Policy')            \n",
    "    print(v_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.rand?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Book method for value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_state_reward(cur_state, action):\n",
    "    if cur_state[0] == 0 and cur_state[1] == 0:\n",
    "        return cur_state, 0  \n",
    "    if cur_state[0] == GRID_DIM-1 and cur_state[1] == GRID_DIM-1:\n",
    "        return cur_state, 0\n",
    "    n_loc = np.zeros(2)\n",
    "#     print(cur_state)\n",
    "#     print(action)\n",
    "    n_loc[0] = cur_state[0] + (action[0])\n",
    "    n_loc[1] = cur_state[1] + action[1]\n",
    "    n_loc = n_loc.astype(int)\n",
    "#     n_loc  = cur_state + action.astype(int)\n",
    "    if n_loc[0] >= GRID_DIM or n_loc[0] < 0 or n_loc[1] >= GRID_DIM or n_loc[1] < 0:\n",
    "        return cur_state, -1\n",
    "    else:\n",
    "        return n_loc, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "[[0. 0. 0. 0.]\n",
      " [2. 0. 0. 3.]\n",
      " [2. 0. 1. 3.]\n",
      " [1. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "GRID_DIM  = 4\n",
    "LEFT, RIGHT, UP , DOWN = np.array([0, -1]), np.array([0, 1]), \\\n",
    "                        np.array([-1, 0]), \\\n",
    "                        np.array([1, 0]) \n",
    "ACTIONS = [LEFT, RIGHT, UP , DOWN]\n",
    "\n",
    "Pi_as = 0.25\n",
    "\n",
    "# V = -np.abs(np.random.rand(GRID_DIM, GRID_DIM))\n",
    "V = np.zeros((GRID_DIM, GRID_DIM))\n",
    "V[0][0] = 0\n",
    "V[GRID_DIM-1][GRID_DIM-1] = 0\n",
    "\n",
    "count = 0\n",
    "while True:\n",
    "#     print(count)\n",
    "    delta = 0\n",
    "#     v_pi_ = np.zeros((GRID_DIM, GRID_DIM))\n",
    "    for i in range(GRID_DIM):\n",
    "        for j in range(GRID_DIM):\n",
    "            if (i, j) != (0,0) and (i, j) != (3,3):\n",
    "                v = V[i,j]\n",
    "                values = [] \n",
    "                for _, act in enumerate(ACTIONS):\n",
    "                    cur_loc = np.array([i, j])\n",
    "\n",
    "                    st, rew = get_state_reward(cur_loc, act)\n",
    "\n",
    "                    new_v = (rew + V[st[0], st[1]])\n",
    "#                     print(st, cur_loc, rew, act, v)\n",
    "                    values.append(new_v)\n",
    "\n",
    "                V[i,j] = max(values)\n",
    "\n",
    "                delta = max(delta, abs(v - V[i,j]))\n",
    "        \n",
    "    count +=1\n",
    "    if delta < 1e-4:\n",
    "        \n",
    "        break\n",
    "\n",
    "            \n",
    "print(V)\n",
    "actions_grid = np.zeros((GRID_DIM, GRID_DIM))\n",
    "for i in range(GRID_DIM):\n",
    "    for j in range(GRID_DIM):\n",
    "        values = []\n",
    "        for act in ACTIONS:\n",
    "            cur_loc = np.array([i, j])\n",
    "            st, rew = get_state_reward(cur_loc, act)\n",
    "            values.append(rew + V[st[0], st[1]])\n",
    "        actions_grid[i,j] = np.argmax(values)\n",
    "print(actions_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0, 1, 2, 3] | [1] | [1] | [0, 1, 2, 3] | \n",
      "[2] | [1] | [1] | [0, 1, 2, 3] | \n",
      "[2] | [1] | [1] | [0, 1, 2, 3] | \n",
      "[1, 2] | [1] | [1] | [0, 1, 2, 3] | "
     ]
    }
   ],
   "source": [
    "p_dash = []\n",
    "for i in range(GRID_DIM):\n",
    "    for j in range(GRID_DIM):\n",
    "        old_action = pi_[i, j]\n",
    "        values = []\n",
    "        cur_loc = np.array([i,j])\n",
    "        \n",
    "        vp  = []\n",
    "        for action in ACTIONS:\n",
    "            st, rew = get_state_reward(cur_loc, action)\n",
    "            values.append(rew + V_s[st[0], st[1]])\n",
    "        max_value = max(values)\n",
    "        for i , value in enumerate(values):\n",
    "            if value == max_value:\n",
    "                vp.append(i)\n",
    "        \n",
    "        p_dash.append(vp)\n",
    "        \n",
    "    \n",
    "for i in range(GRID_DIM):\n",
    "    print()\n",
    "    for j in range(GRID_DIM):\n",
    "        print(p_dash[i*GRID_DIM + j], '|' , end =\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
